{"cells":[{"cell_type":"markdown","metadata":{"id":"yPpALQXdfRme"},"source":["# **KAIST EE**\n","#  **[EE488] note 04A. Slime Volleyball (using a reinforcement learning library)**\n","## **Instructor**: Prof. Yoon, Young-Gyu (ygyoon@kaist.ac.kr)\n","### This example code was prepared by EE488 teaching assistants Han, Seungjae (jay0118@kaist.ac.kr) \u0026 Shin, Changyeop (scey26@kaist.ac.kr).\n","\n","- Class Date : 22.03.xx.\n","- Office Hour : -----------\n","- If you have any questions, ask via KLMS Q\u0026A board or come to TA office hour to get our help."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZEyMhtfQAYg"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"QURAQPWybY-n"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"uUicIf1hbi9O"},"source":["### 0. In this example, we will train an agent that plays **Slime Volleyball**  using  Stable Baselines3. The agent will learn the strategy by playing the game **against the AI embedded in the library**.\n"]},{"cell_type":"markdown","metadata":{"id":"BWc9Y_c6biuh"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"FhUnjuzKbbhW"},"source":["\n","\n","\n","\n","**SlimeVolleyGym** is a simple gym environment for testing single and multi-agent reinforcement learning algorithms.\n","\n","The game is very simple: the agent's goal is to get the ball to land on the ground of its opponent's side, causing its opponent to lose a life. Each agent starts off with five lives. The episode ends when either agent loses all five lives, or after 3000 timesteps has passed. An agent receives a reward of +1 when its opponent loses or -1 when it loses a life.\n","\n","You can even try playing the game by yourself here.\n","https://www.cwest.net/games/slime-volleyball/\n","\n","More detailed explanation about the game can be found here.\n","https://github.com/hardmaru/slimevolleygym\n","\n","**Stable Baselines3**\n","\n","Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines.\n","\n","Github repository: https://github.com/DLR-RM/stable-baselines3\n","\n","Paper: https://jmlr.org/papers/volume22/20-1364/20-1364.pdf\n","https://stable-baselines3.readthedocs.io/en/master/\n","\n","Manual: https://stable-baselines3.readthedocs.io/_/downloads/en/master/pdf/"]},{"cell_type":"markdown","metadata":{"id":"2hff-HQlfWIb"},"source":["### 1. Let's Install **stable-baselines3** and **SlimeVolleyGym**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OYp0aGeRcrN"},"outputs":[],"source":["!pip install stable-baselines3[extra]\n","!pip install slimevolleygym"]},{"cell_type":"markdown","metadata":{"id":"r_zEBAyucw9l"},"source":["### 2. Let's **mount Google drive** so that we can save files as we need."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFgA9e4LcxUh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import sys\n","import os\n","\n","filepath = \"/content/drive/My Drive/trained_agent/A2C/final_ys_trial_1/\"  # Change this path for your Google drive setting.\n","sys.path.append(filepath)\n","experiment = \"Volleyball_04\"\n","\n","if not os.path.exists(f\"{filepath}\"):\n","    os.makedirs(f\"{filepath}\")\n","if not os.path.exists(f\"{filepath}/{experiment}\"):\n","    os.makedirs(f\"{filepath}/{experiment}\")"]},{"cell_type":"markdown","metadata":{"id":"WWFlxXSZgXkM"},"source":["\n","\n","### 3. Let's install a few more things to setup RL enviroment. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DOgfq9yY8Tb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'apt autoremove' to remove them.\n","The following additional packages will be installed:\n","  libxxf86dga1\n","Suggested packages:\n","  mesa-utils\n","The following NEW packages will be installed:\n","  libxxf86dga1 x11-utils xvfb\n","0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\n","Need to get 993 kB of archives.\n","After this operation, 2,982 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n","Fetched 993 kB in 0s (2,609 kB/s)\n","Selecting previously unselected package libxxf86dga1:amd64.\n","(Reading database ... 155202 files and directories currently installed.)\n","Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n","Unpacking x11-utils (7.7+3build1) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Setting up x11-utils (7.7+3build1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Collecting gym[box2d]==0.17.*\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 5.5 MB/s \n","\u001b[?25hCollecting pyvirtualdisplay==0.2.*\n","  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.6)\n","Collecting PyOpenGL-accelerate==3.1.*\n","  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n","\u001b[K     |████████████████████████████████| 538 kB 47.6 MB/s \n","\u001b[?25hCollecting EasyProcess\n","  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n","Requirement already satisfied: numpy\u003e=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.21.6)\n","Requirement already satisfied: pyglet\u003c=1.5.0,\u003e=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n","Requirement already satisfied: cloudpickle\u003c1.7.0,\u003e=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n","Collecting box2d-py~=2.3.5\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 59.8 MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet\u003c=1.5.0,\u003e=1.4.0-\u003egym[box2d]==0.17.*) (0.16.0)\n","Building wheels for collected packages: PyOpenGL-accelerate, gym\n","  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599492 sha256=4eaa96f6b59a59a0d827dbbdca4dadbb0353ad878e2c197b053b2b5cdf19d8b2\n","  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654653 sha256=b3522d9f9830f33fab8e1b507b7c01685a82dc8465fdf1aaea6e6c9888a258d4\n","  Stored in directory: /root/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n","Successfully built PyOpenGL-accelerate gym\n","Installing collected packages: gym, EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.21.0\n","    Uninstalling gym-0.21.0:\n","      Successfully uninstalled gym-0.21.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","stable-baselines3 1.5.0 requires gym==0.21, but you have gym 0.17.3 which is incompatible.\u001b[0m\n","Successfully installed EasyProcess-1.1 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 gym-0.17.3 pyvirtualdisplay-0.2.5\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (0.2.5)\n","Requirement already satisfied: pyglet\u003c=1.5.0,\u003e=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: cloudpickle\u003c1.7.0,\u003e=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: numpy\u003e=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet\u003c=1.5.0,\u003e=1.4.0-\u003egym) (0.16.0)\n","Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (1.1)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'apt autoremove' to remove them.\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl\n","0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n","Need to get 496 kB of archives.\n","After this operation, 5,416 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Fetched 496 kB in 0s (1,467 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 155263 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n"]}],"source":["# install required system dependencies\n","!apt-get install -y xvfb x11-utils\n","\n","# install required python dependencies (might need to install additional gym extras depending)\n","!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n","\n","# Followings are for visualization (as we are running on server without a physical display!)\n","!pip install gym pyvirtualdisplay\n","!apt-get install -y xvfb python-opengl ffmpeg\n","\n","# Autoreload in case that the custom modules are changed\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"xBddaj_pdGsB"},"source":["### 4.  And then, let's **import the libraries** (including SlimeVolleyGym and stable-baselines3) we need to use."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eqfNk0cqsPC9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/cmd_util.py:6: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n","  \"Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\", FutureWarning\n"]}],"source":["import slimevolleygym\n","from slimevolleygym import FrameStack\n","import gym\n","import time\n","\n","import base64\n","\n","from stable_baselines3.common.atari_wrappers import ClipRewardEnv, NoopResetEnv, MaxAndSkipEnv, WarpFrame\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common import results_plotter\n","from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n","from stable_baselines3.common.callbacks import BaseCallback\n","from stable_baselines3.common.callbacks import EvalCallback\n","from stable_baselines3 import A2C, DQN, PPO\n","from stable_baselines3.common.vec_env import VecFrameStack\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import VecNormalize\n","from stable_baselines3.common.cmd_util import make_vec_env"]},{"cell_type":"markdown","metadata":{"id":"QX1kSNMOghDC"},"source":["### 5. We need a bunch of stuff for **visualization** (as we are running on server without a physical display!). Let's not worry about the details here."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EIej4o-EWEiY"},"outputs":[{"data":{"text/plain":["\u003cDisplay cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False\u003e"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from stable_baselines3.common.monitor import Monitor\n","from gym.wrappers import Monitor as eval_Monitor\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML, clear_output\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display\n","\n","display = Display(visible=0, size=(400, 300))\n","display.start()"]},{"cell_type":"markdown","metadata":{"id":"LfZZeAAndb5z"},"source":["### 6. Here's our function for visualization of the result as a **video**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2JGiVirydbEu"},"outputs":[],"source":["def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) \u003e 0:\n","    os.system('rm -f video/compressed.mp4')\n","    time.sleep(1)\n","    mp4 = mp4list[0]\n","    os.system(f\"ffmpeg -i {mp4} -vcodec libx264 video/compressed.mp4\")\n","    video = io.open('video/compressed.mp4', 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''\u003cvideo alt=\"test\" autoplay \n","                loop controls style=\"height: 200px;\"\u003e\n","                \u003csource src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /\u003e\n","             \u003c/video\u003e'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","\n","def wrap_env(env):\n","    env = eval_Monitor(env, './video', force=True)\n","    return env"]},{"cell_type":"markdown","metadata":{"id":"GokIHCekpFGy"},"source":["### 7. Here's a function to **check the training process** and save the intermediate models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ojVxkxbTWFtJ"},"outputs":[],"source":["class SaveOnBestTrainingRewardCallback(BaseCallback):\n","    \"\"\"\n","    Callback for saving a model (the check is done every ``check_freq`` steps)\n","    based on the training reward (in practice, we recommend using ``EvalCallback``).\n","\n","    :param check_freq:\n","    :param log_dir: Path to the folder where the model will be saved.\n","      It must contains the file created by the ``Monitor`` wrapper.\n","    :param verbose: Verbosity level.\n","    \"\"\"\n","    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n","        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.log_dir = log_dir\n","        self.save_path = os.path.join(log_dir, 'best_model')\n","        self.best_mean_reward = -np.inf\n","\n","    def _init_callback(self) -\u003e None:\n","        # Create folder if needed\n","        if self.save_path is not None:\n","            os.makedirs(self.save_path, exist_ok=True)\n","\n","    def _on_step(self) -\u003e bool:\n","        if self.n_calls % self.check_freq == 0:\n","\n","          # Retrieve training reward\n","          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n","          if len(x) \u003e 0:\n","              # Mean training reward over the last 100 episodes\n","              mean_reward = np.mean(y[-100:])\n","              if self.verbose \u003e 0:\n","                print(f\"Num timesteps: {self.num_timesteps}\")\n","                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n","\n","              # New best model, you could save the agent here\n","              if mean_reward \u003e self.best_mean_reward:\n","                  self.best_mean_reward = mean_reward\n","                  # Example for saving best model\n","                  if self.verbose \u003e 0:\n","                    print(f\"Saving new best model to {self.save_path}\")\n","                  self.model.save(self.save_path)\n","\n","        return True"]},{"cell_type":"markdown","metadata":{"id":"Tni-Xz9jOMJX"},"source":["### 8. Let's start **training using PPO algorithm**. You can easily change the hyperparameters if you want. See the manual below. \n","\n","https://stable-baselines3.readthedocs.io/_/downloads/en/master/pdf/\n","\n","Below shows the list of hyperparameters we can change.\n","\n","**PPO**(policy, env, learning_rate=0.0003, n_steps=2048, batch_size=64,\n","n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2,\n","clip_range_vf=None, normalize_advantage=True, ent_coef=0.0,\n","vf_coef=0.5, max_grad_norm=0.5, use_sde=False, sde_sample_freq=- 1,\n","target_kl=None, tensorboard_log=None, create_eval_env=False,\n","policy_kwargs=None, verbose=0, seed=None, device='auto',\n","_init_setup_model=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iOhh5GM-seDP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n","Logging to //content/drive/My Drive/trained_agent/A2C/final_ys_trial_1/a2c_slimevolleyball_tensorboard/A2C_1\n","Num timesteps: 1280000\n","Best mean reward: -inf - Last mean reward per episode: -4.91\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 2560000\n","Best mean reward: -4.91 - Last mean reward per episode: -4.79\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 3840000\n","Best mean reward: -4.79 - Last mean reward per episode: -4.76\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 5120000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.81\n","Num timesteps: 6400000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.86\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 639       |\n","|    ep_rew_mean        | -4.86     |\n","| time/                 |           |\n","|    fps                | 9542      |\n","|    iterations         | 10000     |\n","|    time_elapsed       | 670       |\n","|    total_timesteps    | 6400000   |\n","| train/                |           |\n","|    entropy_loss       | -0.282    |\n","|    explained_variance | 0.945     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 9999      |\n","|    policy_loss        | -7.78e-05 |\n","|    value_loss         | 0.00518   |\n","-------------------------------------\n","Num timesteps: 7680000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.81\n","Num timesteps: 8960000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.82\n","Num timesteps: 10240000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.85\n","Num timesteps: 11520000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.80\n","Num timesteps: 12800000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.79\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 665      |\n","|    ep_rew_mean        | -4.79    |\n","| time/                 |          |\n","|    fps                | 9537     |\n","|    iterations         | 20000    |\n","|    time_elapsed       | 1342     |\n","|    total_timesteps    | 12800000 |\n","| train/                |          |\n","|    entropy_loss       | -0.42    |\n","|    explained_variance | 0.922    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 19999    |\n","|    policy_loss        | 0.00561  |\n","|    value_loss         | 0.00759  |\n","------------------------------------\n","Num timesteps: 14080000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.87\n","Num timesteps: 15360000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.79\n","Num timesteps: 16640000\n","Best mean reward: -4.76 - Last mean reward per episode: -4.74\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 17920000\n","Best mean reward: -4.74 - Last mean reward per episode: -4.86\n","Num timesteps: 19200000\n","Best mean reward: -4.74 - Last mean reward per episode: -4.72\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 693      |\n","|    ep_rew_mean        | -4.72    |\n","| time/                 |          |\n","|    fps                | 9532     |\n","|    iterations         | 30000    |\n","|    time_elapsed       | 2014     |\n","|    total_timesteps    | 19200000 |\n","| train/                |          |\n","|    entropy_loss       | -0.331   |\n","|    explained_variance | 0.911    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 29999    |\n","|    policy_loss        | 0.00339  |\n","|    value_loss         | 0.0141   |\n","------------------------------------\n","Num timesteps: 20480000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.82\n","Num timesteps: 21760000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.84\n","Num timesteps: 23040000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.74\n","Num timesteps: 24320000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.73\n","Num timesteps: 25600000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.82\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 650      |\n","|    ep_rew_mean        | -4.82    |\n","| time/                 |          |\n","|    fps                | 9518     |\n","|    iterations         | 40000    |\n","|    time_elapsed       | 2689     |\n","|    total_timesteps    | 25600000 |\n","| train/                |          |\n","|    entropy_loss       | -0.201   |\n","|    explained_variance | 0.957    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 39999    |\n","|    policy_loss        | 0.000385 |\n","|    value_loss         | 0.0048   |\n","------------------------------------\n","Num timesteps: 26880000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.80\n","Num timesteps: 28160000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.84\n","Num timesteps: 29440000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.87\n","Num timesteps: 30720000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","Num timesteps: 32000000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.78\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 679       |\n","|    ep_rew_mean        | -4.78     |\n","| time/                 |           |\n","|    fps                | 9523      |\n","|    iterations         | 50000     |\n","|    time_elapsed       | 3360      |\n","|    total_timesteps    | 32000000  |\n","| train/                |           |\n","|    entropy_loss       | -0.228    |\n","|    explained_variance | 0.879     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 49999     |\n","|    policy_loss        | -0.000292 |\n","|    value_loss         | 0.0178    |\n","-------------------------------------\n","Num timesteps: 33280000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.73\n","Num timesteps: 34560000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.83\n","Num timesteps: 35840000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.80\n","Num timesteps: 37120000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","Num timesteps: 38400000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.84\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 639      |\n","|    ep_rew_mean        | -4.84    |\n","| time/                 |          |\n","|    fps                | 9513     |\n","|    iterations         | 60000    |\n","|    time_elapsed       | 4036     |\n","|    total_timesteps    | 38400000 |\n","| train/                |          |\n","|    entropy_loss       | -0.322   |\n","|    explained_variance | 0.903    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 59999    |\n","|    policy_loss        | 0.00372  |\n","|    value_loss         | 0.00778  |\n","------------------------------------\n","Num timesteps: 39680000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","Num timesteps: 40960000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.84\n","Num timesteps: 42240000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.76\n","Num timesteps: 43520000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","Num timesteps: 44800000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 652      |\n","|    ep_rew_mean        | -4.81    |\n","| time/                 |          |\n","|    fps                | 9500     |\n","|    iterations         | 70000    |\n","|    time_elapsed       | 4715     |\n","|    total_timesteps    | 44800000 |\n","| train/                |          |\n","|    entropy_loss       | -0.271   |\n","|    explained_variance | 0.916    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 69999    |\n","|    policy_loss        | 0.000457 |\n","|    value_loss         | 0.00896  |\n","------------------------------------\n","Num timesteps: 46080000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.89\n","Num timesteps: 47360000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","Num timesteps: 48640000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.81\n","Num timesteps: 49920000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.79\n","Num timesteps: 51200000\n","Best mean reward: -4.72 - Last mean reward per episode: -4.79\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.13e+03 |\n","|    ep_rew_mean        | -4.79    |\n","| time/                 |          |\n","|    fps                | 9494     |\n","|    iterations         | 80000    |\n","|    time_elapsed       | 5392     |\n","|    total_timesteps    | 51200000 |\n","| train/                |          |\n","|    entropy_loss       | -0.505   |\n","|    explained_variance | 0.887    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 79999    |\n","|    policy_loss        | 0.00585  |\n","|    value_loss         | 0.00535  |\n","------------------------------------\n","Num timesteps: 52480000\n","Best mean reward: -4.72 - Last mean reward per episode: -3.22\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 53760000\n","Best mean reward: -3.22 - Last mean reward per episode: -1.24\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 55040000\n","Best mean reward: -1.24 - Last mean reward per episode: -0.66\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 56320000\n","Best mean reward: -0.66 - Last mean reward per episode: -0.45\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 57600000\n","Best mean reward: -0.45 - Last mean reward per episode: -0.21\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 3e+03    |\n","|    ep_rew_mean        | -0.21    |\n","| time/                 |          |\n","|    fps                | 9496     |\n","|    iterations         | 90000    |\n","|    time_elapsed       | 6065     |\n","|    total_timesteps    | 57600000 |\n","| train/                |          |\n","|    entropy_loss       | -0.596   |\n","|    explained_variance | 0.714    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 89999    |\n","|    policy_loss        | 0.000208 |\n","|    value_loss         | 0.000275 |\n","------------------------------------\n","Num timesteps: 58880000\n","Best mean reward: -0.21 - Last mean reward per episode: -0.08\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 60160000\n","Best mean reward: -0.08 - Last mean reward per episode: -0.07\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 61440000\n","Best mean reward: -0.07 - Last mean reward per episode: -0.02\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 62720000\n","Best mean reward: -0.02 - Last mean reward per episode: -0.20\n","Num timesteps: 64000000\n","Best mean reward: -0.02 - Last mean reward per episode: -0.06\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | -0.06     |\n","| time/                 |           |\n","|    fps                | 9499      |\n","|    iterations         | 100000    |\n","|    time_elapsed       | 6737      |\n","|    total_timesteps    | 64000000  |\n","| train/                |           |\n","|    entropy_loss       | -0.514    |\n","|    explained_variance | 0.799     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 99999     |\n","|    policy_loss        | -0.000722 |\n","|    value_loss         | 5.07e-05  |\n","-------------------------------------\n","Num timesteps: 65280000\n","Best mean reward: -0.02 - Last mean reward per episode: 0.13\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 66560000\n","Best mean reward: 0.13 - Last mean reward per episode: -0.09\n","Num timesteps: 67840000\n","Best mean reward: 0.13 - Last mean reward per episode: 0.03\n","Num timesteps: 69120000\n","Best mean reward: 0.13 - Last mean reward per episode: 0.02\n","Num timesteps: 70400000\n","Best mean reward: 0.13 - Last mean reward per episode: -0.08\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | -0.08     |\n","| time/                 |           |\n","|    fps                | 9501      |\n","|    iterations         | 110000    |\n","|    time_elapsed       | 7409      |\n","|    total_timesteps    | 70400000  |\n","| train/                |           |\n","|    entropy_loss       | -0.428    |\n","|    explained_variance | 0.654     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 109999    |\n","|    policy_loss        | -3.77e-05 |\n","|    value_loss         | 1.87e-05  |\n","-------------------------------------\n","Num timesteps: 71680000\n","Best mean reward: 0.13 - Last mean reward per episode: -0.02\n","Num timesteps: 72960000\n","Best mean reward: 0.13 - Last mean reward per episode: 0.02\n","Num timesteps: 74240000\n","Best mean reward: 0.13 - Last mean reward per episode: 0.13\n","Num timesteps: 75520000\n","Best mean reward: 0.13 - Last mean reward per episode: 0.01\n","Num timesteps: 76800000\n","Best mean reward: 0.13 - Last mean reward per episode: -0.03\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | -0.03     |\n","| time/                 |           |\n","|    fps                | 9503      |\n","|    iterations         | 120000    |\n","|    time_elapsed       | 8081      |\n","|    total_timesteps    | 76800000  |\n","| train/                |           |\n","|    entropy_loss       | -0.435    |\n","|    explained_variance | 0.629     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 119999    |\n","|    policy_loss        | -0.000184 |\n","|    value_loss         | 0.00181   |\n","-------------------------------------\n","Num timesteps: 78080000\n","Best mean reward: 0.13 - Last mean reward per episode: 0.20\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 79360000\n","Best mean reward: 0.20 - Last mean reward per episode: -0.07\n","Num timesteps: 80640000\n","Best mean reward: 0.20 - Last mean reward per episode: 0.01\n","Num timesteps: 81920000\n","Best mean reward: 0.20 - Last mean reward per episode: 0.09\n","Num timesteps: 83200000\n","Best mean reward: 0.20 - Last mean reward per episode: 0.06\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 3e+03    |\n","|    ep_rew_mean        | 0.06     |\n","| time/                 |          |\n","|    fps                | 9498     |\n","|    iterations         | 130000   |\n","|    time_elapsed       | 8758     |\n","|    total_timesteps    | 83200000 |\n","| train/                |          |\n","|    entropy_loss       | -0.488   |\n","|    explained_variance | 0.544    |\n","|    learning_rate      | 0.00096  |\n","|    n_updates          | 129999   |\n","|    policy_loss        | 0.000356 |\n","|    value_loss         | 9.86e-05 |\n","------------------------------------\n","Num timesteps: 84480000\n","Best mean reward: 0.20 - Last mean reward per episode: 0.08\n","Num timesteps: 85760000\n","Best mean reward: 0.20 - Last mean reward per episode: 0.25\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 87040000\n","Best mean reward: 0.25 - Last mean reward per episode: 0.08\n","Num timesteps: 88320000\n","Best mean reward: 0.25 - Last mean reward per episode: 0.17\n","Num timesteps: 89600000\n","Best mean reward: 0.25 - Last mean reward per episode: -0.03\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | -0.03     |\n","| time/                 |           |\n","|    fps                | 9497      |\n","|    iterations         | 140000    |\n","|    time_elapsed       | 9434      |\n","|    total_timesteps    | 89600000  |\n","| train/                |           |\n","|    entropy_loss       | -0.462    |\n","|    explained_variance | 0.778     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 139999    |\n","|    policy_loss        | -0.000697 |\n","|    value_loss         | 0.000264  |\n","-------------------------------------\n","Num timesteps: 90880000\n","Best mean reward: 0.25 - Last mean reward per episode: 0.03\n","Num timesteps: 92160000\n","Best mean reward: 0.25 - Last mean reward per episode: -0.82\n","Num timesteps: 93440000\n","Best mean reward: 0.25 - Last mean reward per episode: 0.22\n","Num timesteps: 94720000\n","Best mean reward: 0.25 - Last mean reward per episode: 0.07\n","Num timesteps: 96000000\n","Best mean reward: 0.25 - Last mean reward per episode: 0.30\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.3       |\n","| time/                 |           |\n","|    fps                | 9495      |\n","|    iterations         | 150000    |\n","|    time_elapsed       | 10110     |\n","|    total_timesteps    | 96000000  |\n","| train/                |           |\n","|    entropy_loss       | -0.479    |\n","|    explained_variance | 0.726     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 149999    |\n","|    policy_loss        | -0.000799 |\n","|    value_loss         | 0.000651  |\n","-------------------------------------\n","Num timesteps: 97280000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.24\n","Num timesteps: 98560000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.14\n","Num timesteps: 99840000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.13\n","Num timesteps: 101120000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.24\n","Num timesteps: 102400000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.25\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.25      |\n","| time/                 |           |\n","|    fps                | 9500      |\n","|    iterations         | 160000    |\n","|    time_elapsed       | 10777     |\n","|    total_timesteps    | 102400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.599    |\n","|    explained_variance | 0.761     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 159999    |\n","|    policy_loss        | -0.000223 |\n","|    value_loss         | 2.09e-05  |\n","-------------------------------------\n","Num timesteps: 103680000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.19\n","Num timesteps: 104960000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.19\n","Num timesteps: 106240000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.09\n","Num timesteps: 107520000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.26\n","Num timesteps: 108800000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.23\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.23      |\n","| time/                 |           |\n","|    fps                | 9507      |\n","|    iterations         | 170000    |\n","|    time_elapsed       | 11443     |\n","|    total_timesteps    | 108800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.549    |\n","|    explained_variance | 0.767     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 169999    |\n","|    policy_loss        | -0.000146 |\n","|    value_loss         | 0.00571   |\n","-------------------------------------\n","Num timesteps: 110080000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.20\n","Num timesteps: 111360000\n","Best mean reward: 0.30 - Last mean reward per episode: 0.31\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 112640000\n","Best mean reward: 0.31 - Last mean reward per episode: 0.19\n","Num timesteps: 113920000\n","Best mean reward: 0.31 - Last mean reward per episode: 0.39\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 115200000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.21\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.21      |\n","| time/                 |           |\n","|    fps                | 9513      |\n","|    iterations         | 180000    |\n","|    time_elapsed       | 12109     |\n","|    total_timesteps    | 115200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.657    |\n","|    explained_variance | 0.857     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 179999    |\n","|    policy_loss        | 1.3e-05   |\n","|    value_loss         | 4.73e-06  |\n","-------------------------------------\n","Num timesteps: 116480000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.16\n","Num timesteps: 117760000\n","Best mean reward: 0.39 - Last mean reward per episode: -0.01\n","Num timesteps: 119040000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.27\n","Num timesteps: 120320000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.12\n","Num timesteps: 121600000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.11\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.11      |\n","| time/                 |           |\n","|    fps                | 9517      |\n","|    iterations         | 190000    |\n","|    time_elapsed       | 12776     |\n","|    total_timesteps    | 121600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.607    |\n","|    explained_variance | 0.894     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 189999    |\n","|    policy_loss        | -0.000351 |\n","|    value_loss         | 3.05e-05  |\n","-------------------------------------\n","Num timesteps: 122880000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.23\n","Num timesteps: 124160000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.34\n","Num timesteps: 125440000\n","Best mean reward: 0.39 - Last mean reward per episode: 0.42\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 126720000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.27\n","Num timesteps: 128000000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.27\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.27      |\n","| time/                 |           |\n","|    fps                | 9519      |\n","|    iterations         | 200000    |\n","|    time_elapsed       | 13445     |\n","|    total_timesteps    | 128000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.695    |\n","|    explained_variance | 0.828     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 199999    |\n","|    policy_loss        | -0.000377 |\n","|    value_loss         | 2.42e-05  |\n","-------------------------------------\n","Num timesteps: 129280000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.18\n","Num timesteps: 130560000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.19\n","Num timesteps: 131840000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.36\n","Num timesteps: 133120000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.29\n","Num timesteps: 134400000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.28\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.28      |\n","| time/                 |           |\n","|    fps                | 9518      |\n","|    iterations         | 210000    |\n","|    time_elapsed       | 14119     |\n","|    total_timesteps    | 134400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.583    |\n","|    explained_variance | 0.957     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 209999    |\n","|    policy_loss        | 0.000556  |\n","|    value_loss         | 0.000165  |\n","-------------------------------------\n","Num timesteps: 135680000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.24\n","Num timesteps: 136960000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.10\n","Num timesteps: 138240000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.33\n","Num timesteps: 139520000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.27\n","Num timesteps: 140800000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.40\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.4       |\n","| time/                 |           |\n","|    fps                | 9520      |\n","|    iterations         | 220000    |\n","|    time_elapsed       | 14789     |\n","|    total_timesteps    | 140800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.627    |\n","|    explained_variance | 0.936     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 219999    |\n","|    policy_loss        | -0.000374 |\n","|    value_loss         | 0.000176  |\n","-------------------------------------\n","Num timesteps: 142080000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.31\n","Num timesteps: 143360000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.11\n","Num timesteps: 144640000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.22\n","Num timesteps: 145920000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.03\n","Num timesteps: 147200000\n","Best mean reward: 0.42 - Last mean reward per episode: -0.29\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | -0.29     |\n","| time/                 |           |\n","|    fps                | 9521      |\n","|    iterations         | 230000    |\n","|    time_elapsed       | 15459     |\n","|    total_timesteps    | 147200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.497    |\n","|    explained_variance | 0.784     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 229999    |\n","|    policy_loss        | 5.17e-05  |\n","|    value_loss         | 0.000467  |\n","-------------------------------------\n","Num timesteps: 148480000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.36\n","Num timesteps: 149760000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.37\n","Num timesteps: 151040000\n","Best mean reward: 0.42 - Last mean reward per episode: 0.45\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 152320000\n","Best mean reward: 0.45 - Last mean reward per episode: 0.39\n","Num timesteps: 153600000\n","Best mean reward: 0.45 - Last mean reward per episode: 0.29\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.29      |\n","| time/                 |           |\n","|    fps                | 9523      |\n","|    iterations         | 240000    |\n","|    time_elapsed       | 16128     |\n","|    total_timesteps    | 153600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.511    |\n","|    explained_variance | 0.98      |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 239999    |\n","|    policy_loss        | -0.000561 |\n","|    value_loss         | 0.000102  |\n","-------------------------------------\n","Num timesteps: 154880000\n","Best mean reward: 0.45 - Last mean reward per episode: 0.49\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 156160000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.28\n","Num timesteps: 157440000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.33\n","Num timesteps: 158720000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.42\n","Num timesteps: 160000000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.28\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.28      |\n","| time/                 |           |\n","|    fps                | 9527      |\n","|    iterations         | 250000    |\n","|    time_elapsed       | 16793     |\n","|    total_timesteps    | 160000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.548    |\n","|    explained_variance | 0.797     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 249999    |\n","|    policy_loss        | 9.08e-05  |\n","|    value_loss         | 7.12e-05  |\n","-------------------------------------\n","Num timesteps: 161280000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.34\n","Num timesteps: 162560000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.39\n","Num timesteps: 163840000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.31\n","Num timesteps: 165120000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.19\n","Num timesteps: 166400000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.45\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.45      |\n","| time/                 |           |\n","|    fps                | 9531      |\n","|    iterations         | 260000    |\n","|    time_elapsed       | 17458     |\n","|    total_timesteps    | 166400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.453    |\n","|    explained_variance | 0.463     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 259999    |\n","|    policy_loss        | 0.000566  |\n","|    value_loss         | 0.00209   |\n","-------------------------------------\n","Num timesteps: 167680000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.35\n","Num timesteps: 168960000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.27\n","Num timesteps: 170240000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.24\n","Num timesteps: 171520000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.41\n","Num timesteps: 172800000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.47\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.47      |\n","| time/                 |           |\n","|    fps                | 9533      |\n","|    iterations         | 270000    |\n","|    time_elapsed       | 18125     |\n","|    total_timesteps    | 172800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.497    |\n","|    explained_variance | 0.862     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 269999    |\n","|    policy_loss        | 0.000514  |\n","|    value_loss         | 2.19e-05  |\n","-------------------------------------\n","Num timesteps: 174080000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.30\n","Num timesteps: 175360000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.33\n","Num timesteps: 176640000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.32\n","Num timesteps: 177920000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.28\n","Num timesteps: 179200000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.47\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.47      |\n","| time/                 |           |\n","|    fps                | 9531      |\n","|    iterations         | 280000    |\n","|    time_elapsed       | 18801     |\n","|    total_timesteps    | 179200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.479    |\n","|    explained_variance | 0.551     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 279999    |\n","|    policy_loss        | -0.00119  |\n","|    value_loss         | 0.00155   |\n","-------------------------------------\n","Num timesteps: 180480000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.44\n","Num timesteps: 181760000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.30\n","Num timesteps: 183040000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.49\n","Num timesteps: 184320000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.34\n","Num timesteps: 185600000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.43\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.43      |\n","| time/                 |           |\n","|    fps                | 9528      |\n","|    iterations         | 290000    |\n","|    time_elapsed       | 19478     |\n","|    total_timesteps    | 185600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.375    |\n","|    explained_variance | 0.0374    |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 289999    |\n","|    policy_loss        | 0.000224  |\n","|    value_loss         | 0.000118  |\n","-------------------------------------\n","Num timesteps: 186880000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.37\n","Num timesteps: 188160000\n","Best mean reward: 0.49 - Last mean reward per episode: -0.78\n","Num timesteps: 189440000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.35\n","Num timesteps: 190720000\n","Best mean reward: 0.49 - Last mean reward per episode: 0.62\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 192000000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.56\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.56      |\n","| time/                 |           |\n","|    fps                | 9525      |\n","|    iterations         | 300000    |\n","|    time_elapsed       | 20155     |\n","|    total_timesteps    | 192000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.402    |\n","|    explained_variance | 0.805     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 299999    |\n","|    policy_loss        | 0.000146  |\n","|    value_loss         | 0.00118   |\n","-------------------------------------\n","Num timesteps: 193280000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.44\n","Num timesteps: 194560000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.48\n","Num timesteps: 195840000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.39\n","Num timesteps: 197120000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.39\n","Num timesteps: 198400000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.27\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.27      |\n","| time/                 |           |\n","|    fps                | 9522      |\n","|    iterations         | 310000    |\n","|    time_elapsed       | 20833     |\n","|    total_timesteps    | 198400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.418    |\n","|    explained_variance | 0.687     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 309999    |\n","|    policy_loss        | 0.000695  |\n","|    value_loss         | 0.00264   |\n","-------------------------------------\n","Num timesteps: 199680000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.37\n","Num timesteps: 200960000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.48\n","Num timesteps: 202240000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.09\n","Num timesteps: 203520000\n","Best mean reward: 0.62 - Last mean reward per episode: 0.65\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 204800000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.42\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.42      |\n","| time/                 |           |\n","|    fps                | 9520      |\n","|    iterations         | 320000    |\n","|    time_elapsed       | 21510     |\n","|    total_timesteps    | 204800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.385    |\n","|    explained_variance | 0.355     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 319999    |\n","|    policy_loss        | 0.000194  |\n","|    value_loss         | 5.88e-05  |\n","-------------------------------------\n","Num timesteps: 206080000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.43\n","Num timesteps: 207360000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.40\n","Num timesteps: 208640000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.39\n","Num timesteps: 209920000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.35\n","Num timesteps: 211200000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.19\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.19      |\n","| time/                 |           |\n","|    fps                | 9519      |\n","|    iterations         | 330000    |\n","|    time_elapsed       | 22185     |\n","|    total_timesteps    | 211200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.371    |\n","|    explained_variance | 0.686     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 329999    |\n","|    policy_loss        | 0.000174  |\n","|    value_loss         | 7.67e-05  |\n","-------------------------------------\n","Num timesteps: 212480000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.41\n","Num timesteps: 213760000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.40\n","Num timesteps: 215040000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.40\n","Num timesteps: 216320000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.54\n","Num timesteps: 217600000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.37\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.37      |\n","| time/                 |           |\n","|    fps                | 9521      |\n","|    iterations         | 340000    |\n","|    time_elapsed       | 22854     |\n","|    total_timesteps    | 217600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.386    |\n","|    explained_variance | 0.867     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 339999    |\n","|    policy_loss        | 0.000457  |\n","|    value_loss         | 3.93e-05  |\n","-------------------------------------\n","Num timesteps: 218880000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.54\n","Num timesteps: 220160000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.28\n","Num timesteps: 221440000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.48\n","Num timesteps: 222720000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.48\n","Num timesteps: 224000000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.34\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.34      |\n","| time/                 |           |\n","|    fps                | 9524      |\n","|    iterations         | 350000    |\n","|    time_elapsed       | 23519     |\n","|    total_timesteps    | 224000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.457    |\n","|    explained_variance | 0.682     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 349999    |\n","|    policy_loss        | -0.000128 |\n","|    value_loss         | 3.71e-05  |\n","-------------------------------------\n","Num timesteps: 225280000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.48\n","Num timesteps: 226560000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.36\n","Num timesteps: 227840000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.25\n","Num timesteps: 229120000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.63\n","Num timesteps: 230400000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.49\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.49      |\n","| time/                 |           |\n","|    fps                | 9525      |\n","|    iterations         | 360000    |\n","|    time_elapsed       | 24188     |\n","|    total_timesteps    | 230400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.351    |\n","|    explained_variance | 0.585     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 359999    |\n","|    policy_loss        | -0.000118 |\n","|    value_loss         | 0.00039   |\n","-------------------------------------\n","Num timesteps: 231680000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.43\n","Num timesteps: 232960000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.43\n","Num timesteps: 234240000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.34\n","Num timesteps: 235520000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.57\n","Num timesteps: 236800000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.33\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.33      |\n","| time/                 |           |\n","|    fps                | 9528      |\n","|    iterations         | 370000    |\n","|    time_elapsed       | 24852     |\n","|    total_timesteps    | 236800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.417    |\n","|    explained_variance | 0.826     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 369999    |\n","|    policy_loss        | -0.000129 |\n","|    value_loss         | 3.31e-05  |\n","-------------------------------------\n","Num timesteps: 238080000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.48\n","Num timesteps: 239360000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.55\n","Num timesteps: 240640000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.31\n","Num timesteps: 241920000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.42\n","Num timesteps: 243200000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.42\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.42      |\n","| time/                 |           |\n","|    fps                | 9530      |\n","|    iterations         | 380000    |\n","|    time_elapsed       | 25518     |\n","|    total_timesteps    | 243200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.354    |\n","|    explained_variance | 0.837     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 379999    |\n","|    policy_loss        | -0.000217 |\n","|    value_loss         | 3.36e-05  |\n","-------------------------------------\n","Num timesteps: 244480000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.39\n","Num timesteps: 245760000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.52\n","Num timesteps: 247040000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.64\n","Num timesteps: 248320000\n","Best mean reward: 0.65 - Last mean reward per episode: 0.68\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 249600000\n","Best mean reward: 0.68 - Last mean reward per episode: 0.77\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.77      |\n","| time/                 |           |\n","|    fps                | 9529      |\n","|    iterations         | 390000    |\n","|    time_elapsed       | 26192     |\n","|    total_timesteps    | 249600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.328    |\n","|    explained_variance | 0.785     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 389999    |\n","|    policy_loss        | -0.000103 |\n","|    value_loss         | 0.000193  |\n","-------------------------------------\n","Num timesteps: 250880000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.48\n","Num timesteps: 252160000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.45\n","Num timesteps: 253440000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.44\n","Num timesteps: 254720000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.57\n","Num timesteps: 256000000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.62\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.62      |\n","| time/                 |           |\n","|    fps                | 9526      |\n","|    iterations         | 400000    |\n","|    time_elapsed       | 26871     |\n","|    total_timesteps    | 256000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.353    |\n","|    explained_variance | 0.602     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 399999    |\n","|    policy_loss        | -0.000517 |\n","|    value_loss         | 6.06e-05  |\n","-------------------------------------\n","Num timesteps: 257280000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.59\n","Num timesteps: 258560000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.47\n","Num timesteps: 259840000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.65\n","Num timesteps: 261120000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.53\n","Num timesteps: 262400000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.65\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.65      |\n","| time/                 |           |\n","|    fps                | 9522      |\n","|    iterations         | 410000    |\n","|    time_elapsed       | 27555     |\n","|    total_timesteps    | 262400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.31     |\n","|    explained_variance | 0.246     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 409999    |\n","|    policy_loss        | -0.000173 |\n","|    value_loss         | 0.00144   |\n","-------------------------------------\n","Num timesteps: 263680000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.57\n","Num timesteps: 264960000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.62\n","Num timesteps: 266240000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.54\n","Num timesteps: 267520000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.58\n","Num timesteps: 268800000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.52\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.52      |\n","| time/                 |           |\n","|    fps                | 9516      |\n","|    iterations         | 420000    |\n","|    time_elapsed       | 28246     |\n","|    total_timesteps    | 268800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.36     |\n","|    explained_variance | 0.809     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 419999    |\n","|    policy_loss        | -0.00139  |\n","|    value_loss         | 0.00159   |\n","-------------------------------------\n","Num timesteps: 270080000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.39\n","Num timesteps: 271360000\n","Best mean reward: 0.77 - Last mean reward per episode: 0.80\n","Saving new best model to /content/drive/My Drive/trained_agent/A2C/final_ys_trial_1//Volleyball_04/best_model\n","Num timesteps: 272640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.44\n","Num timesteps: 273920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.68\n","Num timesteps: 275200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.23\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 2.99e+03  |\n","|    ep_rew_mean        | 0.23      |\n","| time/                 |           |\n","|    fps                | 9502      |\n","|    iterations         | 430000    |\n","|    time_elapsed       | 28961     |\n","|    total_timesteps    | 275200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.337    |\n","|    explained_variance | 0.785     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 429999    |\n","|    policy_loss        | -0.000231 |\n","|    value_loss         | 0.00184   |\n","-------------------------------------\n","Num timesteps: 276480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.51\n","Num timesteps: 277760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.56\n","Num timesteps: 279040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.44\n","Num timesteps: 280320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.35\n","Num timesteps: 281600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.5       |\n","| time/                 |           |\n","|    fps                | 9506      |\n","|    iterations         | 440000    |\n","|    time_elapsed       | 29622     |\n","|    total_timesteps    | 281600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.312    |\n","|    explained_variance | 0.947     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 439999    |\n","|    policy_loss        | 0.000275  |\n","|    value_loss         | 0.000681  |\n","-------------------------------------\n","Num timesteps: 282880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.38\n","Num timesteps: 284160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 285440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.70\n","Num timesteps: 286720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.65\n","Num timesteps: 288000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.57\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.57      |\n","| time/                 |           |\n","|    fps                | 9513      |\n","|    iterations         | 450000    |\n","|    time_elapsed       | 30273     |\n","|    total_timesteps    | 288000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.309    |\n","|    explained_variance | 0.835     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 449999    |\n","|    policy_loss        | 9.31e-05  |\n","|    value_loss         | 0.000143  |\n","-------------------------------------\n","Num timesteps: 289280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","Num timesteps: 290560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.59\n","Num timesteps: 291840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.24\n","Num timesteps: 293120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.69\n","Num timesteps: 294400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.76\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.76      |\n","| time/                 |           |\n","|    fps                | 9518      |\n","|    iterations         | 460000    |\n","|    time_elapsed       | 30929     |\n","|    total_timesteps    | 294400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.408    |\n","|    explained_variance | 0.375     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 459999    |\n","|    policy_loss        | 0.000143  |\n","|    value_loss         | 6.52e-05  |\n","-------------------------------------\n","Num timesteps: 295680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","Num timesteps: 296960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.44\n","Num timesteps: 298240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 299520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.58\n","Num timesteps: 300800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.46\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.46      |\n","| time/                 |           |\n","|    fps                | 9521      |\n","|    iterations         | 470000    |\n","|    time_elapsed       | 31590     |\n","|    total_timesteps    | 300800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.494    |\n","|    explained_variance | 0.736     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 469999    |\n","|    policy_loss        | -5.71e-05 |\n","|    value_loss         | 0.000284  |\n","-------------------------------------\n","Num timesteps: 302080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 303360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.56\n","Num timesteps: 304640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.59\n","Num timesteps: 305920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.47\n","Num timesteps: 307200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.34\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.34      |\n","| time/                 |           |\n","|    fps                | 9525      |\n","|    iterations         | 480000    |\n","|    time_elapsed       | 32248     |\n","|    total_timesteps    | 307200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.268    |\n","|    explained_variance | 0.762     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 479999    |\n","|    policy_loss        | 8.03e-06  |\n","|    value_loss         | 0.00228   |\n","-------------------------------------\n","Num timesteps: 308480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","Num timesteps: 309760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.78\n","Num timesteps: 311040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.32\n","Num timesteps: 312320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.58\n","Num timesteps: 313600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.63      |\n","| time/                 |           |\n","|    fps                | 9528      |\n","|    iterations         | 490000    |\n","|    time_elapsed       | 32913     |\n","|    total_timesteps    | 313600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.32     |\n","|    explained_variance | 0.781     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 489999    |\n","|    policy_loss        | -0.000191 |\n","|    value_loss         | 0.000547  |\n","-------------------------------------\n","Num timesteps: 314880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.34\n","Num timesteps: 316160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.57\n","Num timesteps: 317440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.65\n","Num timesteps: 318720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","Num timesteps: 320000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.39\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.39      |\n","| time/                 |           |\n","|    fps                | 9528      |\n","|    iterations         | 500000    |\n","|    time_elapsed       | 33582     |\n","|    total_timesteps    | 320000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.304    |\n","|    explained_variance | 0.841     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 499999    |\n","|    policy_loss        | -0.000203 |\n","|    value_loss         | 9.53e-05  |\n","-------------------------------------\n","Num timesteps: 321280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.54\n","Num timesteps: 322560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.49\n","Num timesteps: 323840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.57\n","Num timesteps: 325120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 326400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.74\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.74      |\n","| time/                 |           |\n","|    fps                | 9530      |\n","|    iterations         | 510000    |\n","|    time_elapsed       | 34248     |\n","|    total_timesteps    | 326400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.278    |\n","|    explained_variance | 0.455     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 509999    |\n","|    policy_loss        | 7.83e-05  |\n","|    value_loss         | 0.00198   |\n","-------------------------------------\n","Num timesteps: 327680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.21\n","Num timesteps: 328960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.54\n","Num timesteps: 330240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.53\n","Num timesteps: 331520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.41\n","Num timesteps: 332800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.31\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.31      |\n","| time/                 |           |\n","|    fps                | 9530      |\n","|    iterations         | 520000    |\n","|    time_elapsed       | 34919     |\n","|    total_timesteps    | 332800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.323    |\n","|    explained_variance | -1.84     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 519999    |\n","|    policy_loss        | -4.64e-05 |\n","|    value_loss         | 0.00213   |\n","-------------------------------------\n","Num timesteps: 334080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.49\n","Num timesteps: 335360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.38\n","Num timesteps: 336640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","Num timesteps: 337920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 339200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.57\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.57      |\n","| time/                 |           |\n","|    fps                | 9527      |\n","|    iterations         | 530000    |\n","|    time_elapsed       | 35600     |\n","|    total_timesteps    | 339200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.282    |\n","|    explained_variance | 0.614     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 529999    |\n","|    policy_loss        | 0.000132  |\n","|    value_loss         | 0.00127   |\n","-------------------------------------\n","Num timesteps: 340480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 341760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.42\n","Num timesteps: 343040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.71\n","Num timesteps: 344320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.42\n","Num timesteps: 345600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.61\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.61      |\n","| time/                 |           |\n","|    fps                | 9524      |\n","|    iterations         | 540000    |\n","|    time_elapsed       | 36284     |\n","|    total_timesteps    | 345600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.331    |\n","|    explained_variance | 0.969     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 539999    |\n","|    policy_loss        | 3e-06     |\n","|    value_loss         | 5.59e-05  |\n","-------------------------------------\n","Num timesteps: 346880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 348160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","Num timesteps: 349440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.27\n","Num timesteps: 350720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.59\n","Num timesteps: 352000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.64\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.64      |\n","| time/                 |           |\n","|    fps                | 9521      |\n","|    iterations         | 550000    |\n","|    time_elapsed       | 36970     |\n","|    total_timesteps    | 352000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.33     |\n","|    explained_variance | 0.414     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 549999    |\n","|    policy_loss        | 0.000602  |\n","|    value_loss         | 0.00533   |\n","-------------------------------------\n","Num timesteps: 353280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.69\n","Num timesteps: 354560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.52\n","Num timesteps: 355840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.47\n","Num timesteps: 357120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 358400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.49\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.49      |\n","| time/                 |           |\n","|    fps                | 9519      |\n","|    iterations         | 560000    |\n","|    time_elapsed       | 37647     |\n","|    total_timesteps    | 358400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.404    |\n","|    explained_variance | 0.286     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 559999    |\n","|    policy_loss        | -0.000104 |\n","|    value_loss         | 0.000105  |\n","-------------------------------------\n","Num timesteps: 359680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.41\n","Num timesteps: 360960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.57\n","Num timesteps: 362240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.62\n","Num timesteps: 363520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","Num timesteps: 364800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.46\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.46      |\n","| time/                 |           |\n","|    fps                | 9503      |\n","|    iterations         | 570000    |\n","|    time_elapsed       | 38387     |\n","|    total_timesteps    | 364800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.351    |\n","|    explained_variance | 0.702     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 569999    |\n","|    policy_loss        | -0.000249 |\n","|    value_loss         | 0.000599  |\n","-------------------------------------\n","Num timesteps: 366080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.56\n","Num timesteps: 367360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.35\n","Num timesteps: 368640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.39\n","Num timesteps: 369920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.56\n","Num timesteps: 371200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.56\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.56      |\n","| time/                 |           |\n","|    fps                | 9501      |\n","|    iterations         | 580000    |\n","|    time_elapsed       | 39068     |\n","|    total_timesteps    | 371200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.275    |\n","|    explained_variance | 0.981     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 579999    |\n","|    policy_loss        | -0.000232 |\n","|    value_loss         | 7.56e-05  |\n","-------------------------------------\n","Num timesteps: 372480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.35\n","Num timesteps: 373760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.36\n","Num timesteps: 375040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.43\n","Num timesteps: 376320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.45\n","Num timesteps: 377600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.64\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 2.99e+03  |\n","|    ep_rew_mean        | 0.64      |\n","| time/                 |           |\n","|    fps                | 9502      |\n","|    iterations         | 590000    |\n","|    time_elapsed       | 39737     |\n","|    total_timesteps    | 377600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.295    |\n","|    explained_variance | 0.842     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 589999    |\n","|    policy_loss        | -0.000211 |\n","|    value_loss         | 3.76e-05  |\n","-------------------------------------\n","Num timesteps: 378880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","Num timesteps: 380160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.64\n","Num timesteps: 381440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.43\n","Num timesteps: 382720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.59\n","Num timesteps: 384000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.46\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.46      |\n","| time/                 |           |\n","|    fps                | 9499      |\n","|    iterations         | 600000    |\n","|    time_elapsed       | 40422     |\n","|    total_timesteps    | 384000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.255    |\n","|    explained_variance | 0.977     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 599999    |\n","|    policy_loss        | 3.34e-06  |\n","|    value_loss         | 4.65e-05  |\n","-------------------------------------\n","Num timesteps: 385280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.76\n","Num timesteps: 386560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.76\n","Num timesteps: 387840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.61\n","Num timesteps: 389120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.60\n","Num timesteps: 390400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.27\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.27      |\n","| time/                 |           |\n","|    fps                | 9489      |\n","|    iterations         | 610000    |\n","|    time_elapsed       | 41138     |\n","|    total_timesteps    | 390400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.227    |\n","|    explained_variance | -0.87     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 609999    |\n","|    policy_loss        | -1.92e-05 |\n","|    value_loss         | 7.84e-05  |\n","-------------------------------------\n","Num timesteps: 391680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.47\n","Num timesteps: 392960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.79\n","Num timesteps: 394240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.53\n","Num timesteps: 395520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","Num timesteps: 396800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.55      |\n","| time/                 |           |\n","|    fps                | 9490      |\n","|    iterations         | 620000    |\n","|    time_elapsed       | 41810     |\n","|    total_timesteps    | 396800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.338    |\n","|    explained_variance | 0.759     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 619999    |\n","|    policy_loss        | 1.16e-05  |\n","|    value_loss         | 0.00503   |\n","-------------------------------------\n","Num timesteps: 398080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.77\n","Num timesteps: 399360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.52\n","Num timesteps: 400640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.60\n","Num timesteps: 401920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","Num timesteps: 403200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.64\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.64      |\n","| time/                 |           |\n","|    fps                | 9490      |\n","|    iterations         | 630000    |\n","|    time_elapsed       | 42486     |\n","|    total_timesteps    | 403200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.275    |\n","|    explained_variance | 0.955     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 629999    |\n","|    policy_loss        | -0.00028  |\n","|    value_loss         | 0.000582  |\n","-------------------------------------\n","Num timesteps: 404480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.44\n","Num timesteps: 405760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.37\n","Num timesteps: 407040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.60\n","Num timesteps: 408320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.68\n","Num timesteps: 409600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.60\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.6       |\n","| time/                 |           |\n","|    fps                | 9494      |\n","|    iterations         | 640000    |\n","|    time_elapsed       | 43141     |\n","|    total_timesteps    | 409600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.219    |\n","|    explained_variance | 0.673     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 639999    |\n","|    policy_loss        | 0.000381  |\n","|    value_loss         | 9.57e-05  |\n","-------------------------------------\n","Num timesteps: 410880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","Num timesteps: 412160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.42\n","Num timesteps: 413440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 414720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.28\n","Num timesteps: 416000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.53\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.53      |\n","| time/                 |           |\n","|    fps                | 9493      |\n","|    iterations         | 650000    |\n","|    time_elapsed       | 43818     |\n","|    total_timesteps    | 416000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.153    |\n","|    explained_variance | 0.696     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 649999    |\n","|    policy_loss        | -1.03e-05 |\n","|    value_loss         | 0.000841  |\n","-------------------------------------\n","Num timesteps: 417280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","Num timesteps: 418560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 419840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.28\n","Num timesteps: 421120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.64\n","Num timesteps: 422400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.60\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.6       |\n","| time/                 |           |\n","|    fps                | 9489      |\n","|    iterations         | 660000    |\n","|    time_elapsed       | 44511     |\n","|    total_timesteps    | 422400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.265    |\n","|    explained_variance | 0.902     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 659999    |\n","|    policy_loss        | -0.00013  |\n","|    value_loss         | 0.000101  |\n","-------------------------------------\n","Num timesteps: 423680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.73\n","Num timesteps: 424960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.67\n","Num timesteps: 426240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.49\n","Num timesteps: 427520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.71\n","Num timesteps: 428800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.48      |\n","| time/                 |           |\n","|    fps                | 9486      |\n","|    iterations         | 670000    |\n","|    time_elapsed       | 45199     |\n","|    total_timesteps    | 428800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.197    |\n","|    explained_variance | 0.123     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 669999    |\n","|    policy_loss        | -0.000214 |\n","|    value_loss         | 0.000249  |\n","-------------------------------------\n","Num timesteps: 430080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.66\n","Num timesteps: 431360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.65\n","Num timesteps: 432640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.31\n","Num timesteps: 433920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.46\n","Num timesteps: 435200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.75\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 2.99e+03  |\n","|    ep_rew_mean        | 0.73      |\n","| time/                 |           |\n","|    fps                | 9485      |\n","|    iterations         | 680000    |\n","|    time_elapsed       | 45882     |\n","|    total_timesteps    | 435200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.205    |\n","|    explained_variance | 0.955     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 679999    |\n","|    policy_loss        | 0.000296  |\n","|    value_loss         | 0.000102  |\n","-------------------------------------\n","Num timesteps: 436480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.54\n","Num timesteps: 437760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.67\n","Num timesteps: 439040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.57\n","Num timesteps: 440320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 441600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.79\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.79      |\n","| time/                 |           |\n","|    fps                | 9483      |\n","|    iterations         | 690000    |\n","|    time_elapsed       | 46566     |\n","|    total_timesteps    | 441600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.245    |\n","|    explained_variance | 0.925     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 689999    |\n","|    policy_loss        | -0.000162 |\n","|    value_loss         | 0.000523  |\n","-------------------------------------\n","Num timesteps: 442880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.36\n","Num timesteps: 444160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.61\n","Num timesteps: 445440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","Num timesteps: 446720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.52\n","Num timesteps: 448000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.35\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.35      |\n","| time/                 |           |\n","|    fps                | 9480      |\n","|    iterations         | 700000    |\n","|    time_elapsed       | 47254     |\n","|    total_timesteps    | 448000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.267    |\n","|    explained_variance | -0.958    |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 699999    |\n","|    policy_loss        | 0.000116  |\n","|    value_loss         | 0.000211  |\n","-------------------------------------\n","Num timesteps: 449280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.49\n","Num timesteps: 450560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.58\n","Num timesteps: 451840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","Num timesteps: 453120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.61\n","Num timesteps: 454400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.66\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.66      |\n","| time/                 |           |\n","|    fps                | 9481      |\n","|    iterations         | 710000    |\n","|    time_elapsed       | 47922     |\n","|    total_timesteps    | 454400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.308    |\n","|    explained_variance | 0.848     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 709999    |\n","|    policy_loss        | 0.000159  |\n","|    value_loss         | 8.32e-05  |\n","-------------------------------------\n","Num timesteps: 455680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.68\n","Num timesteps: 456960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.68\n","Num timesteps: 458240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.64\n","Num timesteps: 459520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.58\n","Num timesteps: 460800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.65\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.65      |\n","| time/                 |           |\n","|    fps                | 9482      |\n","|    iterations         | 720000    |\n","|    time_elapsed       | 48593     |\n","|    total_timesteps    | 460800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.255    |\n","|    explained_variance | -0.378    |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 719999    |\n","|    policy_loss        | 6.4e-05   |\n","|    value_loss         | 0.000255  |\n","-------------------------------------\n","Num timesteps: 462080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","Num timesteps: 463360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.62\n","Num timesteps: 464640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.45\n","Num timesteps: 465920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.73\n","Num timesteps: 467200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.11\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.11      |\n","| time/                 |           |\n","|    fps                | 9478      |\n","|    iterations         | 730000    |\n","|    time_elapsed       | 49291     |\n","|    total_timesteps    | 467200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.249    |\n","|    explained_variance | -2.04     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 729999    |\n","|    policy_loss        | 0.00021   |\n","|    value_loss         | 0.00126   |\n","-------------------------------------\n","Num timesteps: 468480000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.62\n","Num timesteps: 469760000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.52\n","Num timesteps: 471040000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.38\n","Num timesteps: 472320000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.44\n","Num timesteps: 473600000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.68\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.68      |\n","| time/                 |           |\n","|    fps                | 9475      |\n","|    iterations         | 740000    |\n","|    time_elapsed       | 49983     |\n","|    total_timesteps    | 473600000 |\n","| train/                |           |\n","|    entropy_loss       | -0.237    |\n","|    explained_variance | 0.798     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 739999    |\n","|    policy_loss        | -0.000148 |\n","|    value_loss         | 0.000119  |\n","-------------------------------------\n","Num timesteps: 474880000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 476160000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.48\n","Num timesteps: 477440000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.54\n","Num timesteps: 478720000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.50\n","Num timesteps: 480000000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.58\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.58      |\n","| time/                 |           |\n","|    fps                | 9470      |\n","|    iterations         | 750000    |\n","|    time_elapsed       | 50686     |\n","|    total_timesteps    | 480000000 |\n","| train/                |           |\n","|    entropy_loss       | -0.367    |\n","|    explained_variance | 0.356     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 749999    |\n","|    policy_loss        | 4.92e-05  |\n","|    value_loss         | 0.00136   |\n","-------------------------------------\n","Num timesteps: 481280000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.54\n","Num timesteps: 482560000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.61\n","Num timesteps: 483840000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.45\n","Num timesteps: 485120000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.40\n","Num timesteps: 486400000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.63\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.63      |\n","| time/                 |           |\n","|    fps                | 9468      |\n","|    iterations         | 760000    |\n","|    time_elapsed       | 51370     |\n","|    total_timesteps    | 486400000 |\n","| train/                |           |\n","|    entropy_loss       | -0.344    |\n","|    explained_variance | 0.984     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 759999    |\n","|    policy_loss        | -0.000211 |\n","|    value_loss         | 0.000115  |\n","-------------------------------------\n","Num timesteps: 487680000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.55\n","Num timesteps: 488960000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.53\n","Num timesteps: 490240000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.46\n","Num timesteps: 491520000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.68\n","Num timesteps: 492800000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.58\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.58      |\n","| time/                 |           |\n","|    fps                | 9464      |\n","|    iterations         | 770000    |\n","|    time_elapsed       | 52067     |\n","|    total_timesteps    | 492800000 |\n","| train/                |           |\n","|    entropy_loss       | -0.333    |\n","|    explained_variance | 0.989     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 769999    |\n","|    policy_loss        | -0.000239 |\n","|    value_loss         | 8.37e-05  |\n","-------------------------------------\n","Num timesteps: 494080000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.42\n","Num timesteps: 495360000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.66\n","Num timesteps: 496640000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.60\n","Num timesteps: 497920000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.46\n","Num timesteps: 499200000\n","Best mean reward: 0.80 - Last mean reward per episode: 0.62\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 3e+03     |\n","|    ep_rew_mean        | 0.62      |\n","| time/                 |           |\n","|    fps                | 9460      |\n","|    iterations         | 780000    |\n","|    time_elapsed       | 52767     |\n","|    total_timesteps    | 499200000 |\n","| train/                |           |\n","|    entropy_loss       | -0.34     |\n","|    explained_variance | 0.737     |\n","|    learning_rate      | 0.00096   |\n","|    n_updates          | 779999    |\n","|    policy_loss        | -8.12e-05 |\n","|    value_loss         | 7.64e-05  |\n","-------------------------------------\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 576x144 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["#self-play START\n","BEST_THRESHOLD = 0.5 # must achieve a mean score above this to replace prev best self\n","NUM_TIMESTEPS = int(5e8)\n","EVAL_FREQ = int(1e5)\n","EVAL_EPISODES = int(1e2)\n","\n","class SelfPlayCallback(EvalCallback):\n","  # hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n","  # after saving model, resets the best score to be BEST_THRESHOLD\n","  def __init__(self, *args, **kwargs):\n","    super(SelfPlayCallback, self).__init__(*args, **kwargs)\n","    self.best_mean_reward = BEST_THRESHOLD\n","    self.generation = 0\n","  def _on_step(self) -\u003e bool:\n","    result = super(SelfPlayCallback, self)._on_step()\n","    if result and self.best_mean_reward \u003e BEST_THRESHOLD:\n","      self.generation += 1\n","      print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n","      print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)\n","      source_file = os.path.join(log_dir, \"best_model.zip\")\n","      backup_file = os.path.join(log_dir, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n","      copyfile(source_file, backup_file)\n","      self.best_mean_reward = BEST_THRESHOLD\n","    return result\n","\n","\n","class SlimeVolleySelfPlayEnv(slimevolleygym.SlimeVolleyEnv):\n","  # wrapper over the normal single player env, but loads the best self play model\n","  def __init__(self):\n","    super(SlimeVolleySelfPlayEnv, self).__init__()\n","    self.policy = self\n","    self.best_model = None\n","    self.best_model_filename = None\n","  def predict(self, obs): # the policy\n","    if self.best_model is None:\n","      return self.action_space.sample() # return a random action\n","    else:\n","      action, _ = self.best_model.predict(obs)\n","      return action\n","  def reset(self):\n","    # load model if it's there\n","    modellist = [f for f in os.listdir(log_dir) if f.startswith(\"history\")]\n","    modellist.sort()\n","    if len(modellist) \u003e 0:\n","      filename = os.path.join(log_dir, modellist[-1]) # the latest best model\n","      if filename != self.best_model_filename:\n","        print(\"loading model: \", filename)\n","        self.best_model_filename = filename\n","        if self.best_model is not None:\n","          del self.best_model\n","        self.best_model = PPO.load(filename, env=self)\n","    return super(SlimeVolleySelfPlayEnv, self).reset()\n","\n","# eval_callback = SelfPlayCallback(env,\n","#     best_model_save_path=log_dir,\n","#     log_path=log_dir,\n","#     eval_freq=EVAL_FREQ,\n","#     n_eval_episodes=EVAL_EPISODES,\n","#     deterministic=False)\n","\n","#self-play END\n","\n","class ObservationWrapper(gym.ObservationWrapper):\n","\n","    def __init__(self, env):\n","        super().__init__(env)\n","\n","    def observation(self, observation):\n","        return observation/(3.4028234663852886e+38)\n","\n","\n","class ActionNoiseWrapper(gym.Wrapper):\n","\n","    def __init__(self, env: gym.Env, noise_std: float = 0.1):\n","        super().__init__(env)\n","        self.noise_std = noise_std\n","\n","    def step(self, action):\n","        noise = np.random.normal(np.zeros_like(action), np.ones_like(action) * self.noise_std)\n","        noisy_action = action + noise\n","        for i in range(len(noisy_action)):\n","          if noisy_action[i] \u003c0.8:\n","            noisy_action[i]=0\n","          else:\n","            noisy_action[i]=1\n","      \n","        return self.env.step(noisy_action)\n","\n","\n","def linear_schedule(initial_value):\n","    \"\"\"\n","    Linear learning rate schedule.\n","    :param initial_value: (float or str)\n","    :return: (function)\n","    \"\"\"\n","    if isinstance(initial_value, str):\n","        initial_value = float(initial_value)\n","\n","    def func(progress_remaining: float) -\u003e float:\n","        \"\"\"\n","        Progress will decrease from 1 (beginning) to 0\n","        :param progress_remaining: (float)\n","        :return: (float)\n","        \"\"\"\n","        return progress_remaining * initial_value\n","\n","    return func\n","\n","def custom_wrapper(env):\n","    #env = gym.make(env_name)\n","    env = ActionNoiseWrapper(env)\n","    env = MaxAndSkipEnv(env, skip=4)\n","    #env = WarpFrame(env)\n","    env = ClipRewardEnv(env)\n","    #env = FrameStack(env, 4)\n","    return env\n","\n","# Create log dir\n","log_dir = f\"{filepath}/{experiment}\"\n","# os.makedirs(log_dir, exist_ok=True)\n","\n","def make_env():\n","    env = make_vec_env(\"SlimeVolley-v0\",n_envs=128,seed=0,monitor_dir=log_dir) # CHANGE SEED HERE EVERY TIME\n","    #env.seed(seed)\n","    return env\n","\n","env = make_env()\n","\n","\n","# # Create and wrap the environment\n","# env = Monitor(make_env(seed=0), log_dir)\n","\n","model = A2C(\"MlpPolicy\", env,learning_rate = 0.00096, verbose=1, tensorboard_log=\"//content/drive/My Drive/trained_agent/A2C/final_ys_trial_1/a2c_slimevolleyball_tensorboard/\")\n","#model = PPO(\"MlpPolicy\", env, n_steps=4096,  batch_size=64,  n_epochs=10, ent_coef=0.0, verbose=1)\n","\n","\n","\n","# Create the callback: check every 1000 steps\n","callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_dir)\n","\n","model.learn(total_timesteps=NUM_TIMESTEPS, callback = callback, eval_freq=10000, log_interval=10000)\n","\n","plot_results([log_dir], NUM_TIMESTEPS, results_plotter.X_TIMESTEPS, \"A2C Volleyball (state mode)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Fpe2eyuygP4F"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive\n"]},{"data":{"application/javascript":["\n","        (async () =\u003e {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "],"text/plain":["\u003cIPython.core.display.Javascript object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["%load_ext tensorboard\n","%cd /content/drive/MyDrive/\n","ts_log_dir = \"/trained_agent/A2C/final_ys_trial_1/a2c_slimevolleyball_tensorboard/\"\n","%tensorboard --logdir {ts_log_dir}"]},{"cell_type":"markdown","metadata":{"id":"tkMnCM9FOVCm"},"source":["### 9. Now, let's **evaluate** the performance of the trained network!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Swn0Iz8Wse99"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  UserWarning,\n"]},{"name":"stdout","output_type":"stream","text":["0.5 0.806225774829855\n"]}],"source":["from stable_baselines3.common.evaluation import evaluate_policy\n","\n","env = gym.make(\"SlimeVolley-v0\")\n","\n","mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n","print(mean_reward, std_reward)\n"]},{"cell_type":"markdown","metadata":{"id":"RRT-F4MntOxY"},"source":["### **Congratulations**! You sucessfully trained your agent that play Slime Volleyball in **state mode**."]},{"cell_type":"markdown","metadata":{"id":"kd8Xf-OftRSV"},"source":["---"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"EE488_04A_Volleyball_statemode_SB3_train_A2C_ys_final_1.ipynb","provenance":[{"file_id":"1pom4lEGf8_YAUhUBfB7t3y51HDv43A6K","timestamp":1651339589681}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}